from typing import Final, NamedTuple, Self
import torch

from deep_attention_visual_odometry.solvers import IOptimisableFunction
from deep_attention_visual_odometry.utils import masked_merge_tensors
from deep_attention_visual_odometry.geometry.lie_rotation import LieRotation


class PinholeCameraModel(IOptimisableFunction):
    """
    A base class for pinhole camera models as optimisable functions.
    Implements a lot of the IOptmisableFunction interface, particularly around add and masked update.
    Provides caching and calculation of intermediate values like world points, camera relative points, u and v.
    Caching is separated from calculation, to provide additional functionality like clamping,
    override the calculate methods.

    Assumes the class will jointly optimise for the 3D positions of the points, the camera intrinsics, and extrinsics.
    Although if a subclass
    Based on,
    "More accurate pinhole camera calibration with imperfect planar target" by
    Klaus H Strobl and Gerd Hirzinger in ICCV 2011

    Supports a batch dimension and multiple parallel estimates,
    so the image coordinates should be shape BxExMxNx2.
    Shape conventions:
    B is batch size, E the number of estimates, M the number of camera views,
    and N the number of points
    """

    class _CameraGradients(NamedTuple):
        partial_du_df: torch.Tensor
        partial_dv_df: torch.Tensor
        partial_du_da: torch.Tensor
        partial_dv_da: torch.Tensor
        partial_du_db: torch.Tensor
        partial_dv_db: torch.Tensor
        partial_du_dc: torch.Tensor
        partial_dv_dc: torch.Tensor
        partial_du_dtx: torch.Tensor
        partial_dv_dty: torch.Tensor
        partial_du_dtz: torch.Tensor
        partial_dv_dtz: torch.Tensor
        partial_du_dx: torch.Tensor
        partial_dv_dx: torch.Tensor
        partial_du_dy: torch.Tensor
        partial_dv_dy: torch.Tensor
        partial_du_dz: torch.Tensor
        partial_dv_dz: torch.Tensor

    # Parameter indices in order
    CX: Final[int] = 0
    CY: Final[int] = 1
    F: Final[int] = 2
    VIEW_START: Final[int] = 3

    def __init__(
        self,
        focal_length: torch.Tensor,
        cx: torch.Tensor,
        cy: torch.Tensor,
        translation: torch.Tensor,
        orientation: LieRotation,
        world_points: torch.Tensor,
        true_projected_points: torch.Tensor,
        visibility_mask: torch.Tensor,
        minimum_z_distance: float = 1e-3,
        maximum_pixel_ratio: float = 5.0,
        constrain: bool = False,
        max_gradient: float = -1.0,
        enable_error_gradients: bool = True,
        enable_grad_gradients: bool = True,
        _error: torch.Tensor | None = None,
        _gradient: torch.Tensor | None = None,
        _error_mask: torch.Tensor | None = None,
        _gradient_mask: torch.Tensor | None = None,
    ):
        """
        :param focal_length: BxE (assumed fixed for all views)
        :param cx: BxE (fixed for all views)
        :param cy: BxE (fixed for all views)
        :param translation: BxExMx3 The translation of each view.
        :param orientation: BxExMx3 The orientation of each view, as an axis times an angle.
        :param world_points: BxEx(N - 2)x3 The world points
        :param true_projected_points: BxMxNx2
        :param visibility_mask: BxMxN Whether a given point appears within a view.
        Missing points have their error ignored.
        :param minimum_z_distance: Minimum distance from the camera plane
        :param maximum_pixel_ratio: The maximum ratio for x/z and y/z.
        Since we expect the image coordinates to range [-1, 1], constraining this keeps the error sane.
        :param enable_error_gradients: If true, the output error tensor may require grad,
        if false, it will be detached.
        :param enable_grad_gradients: If true, the output gradient tensor may require grad,
        if false, it will be detached.
        :param _error: Pre-calculated error values (generated by parent)
        :param _gradient: Pre-calculated gradient values (generated by parent)
        :param _error_mask: Where false, the error value needs to be re-calculated (used by masked_update)
        :param _gradient_mask: Where false, the gradient needs to be re-calculated (used by masked_update)
        """
        self.minimum_z_distance = float(minimum_z_distance)
        self.maximum_pixel_ratio = 1.0 / abs(float(maximum_pixel_ratio))
        self._constrain = bool(constrain)
        self._max_gradient = float(max_gradient)
        self._num_views = true_projected_points.size(1)
        self._num_points = true_projected_points.size(2)
        self._num_estimates = focal_length.size(1)
        self._enable_error_gradients = bool(enable_error_gradients)
        self._enable_grad_gradients = bool(enable_grad_gradients)
        self._error_scale = torch.tensor(
            1.0 / (self._num_views * self._num_points)
        ).sqrt()
        self._focal_length = focal_length
        self._cx = cx
        self._cy = cy
        self._translation = translation
        self._world_points = world_points
        self._true_projected_points = true_projected_points
        self._visibility_mask = visibility_mask
        self._orientation = orientation
        self._camera_relative_points = None
        self._u = None
        self._v = None
        self._error = _error
        self._gradient = _gradient

        self._error_mask = _error_mask
        self._gradient_mask = _gradient_mask

    @property
    def batch_size(self) -> int:
        return self._true_projected_points.size(0)

    @property
    def num_estimates(self) -> int:
        return self._num_estimates

    @property
    def num_parameters(self) -> int:
        return 3 + 6 * self._num_views + 3 * self._num_points - 7

    @property
    def device(self) -> torch.device:
        return self._true_projected_points.device

    @property
    def focal_length(self) -> torch.Tensor:
        return self._focal_length

    @property
    def cx(self) -> torch.Tensor:
        return self._cx

    @property
    def cy(self) -> torch.Tensor:
        return self._cy

    def get_error(self) -> torch.Tensor:
        """
        Get the error.
        Results are cached, so that calling this repeatedly returns the same tensor.
        Actual calculation of the error is delegated to calculate_error
        :returns: Error for each estimate, BxE.
        """
        if self._error is None or self._error_mask is not None:
            u = self.get_u()
            v = self.get_v()
            true_projected_points = self._true_projected_points
            visibility_mask = self._visibility_mask[:, None, :, :].expand_as(u)
            if not self._enable_error_gradients:
                u = u.detach()
                v = v.detach()
            to_update: torch.Tensor | None = None
            if self._error is not None and self._error_mask is not None:
                to_update = torch.logical_not(self._error_mask)
                u = u[to_update].unsqueeze(1)
                v = v[to_update].unsqueeze(1)
                true_projected_points = true_projected_points.unsqueeze(1).expand(-1, self.num_estimates, -1, -1, -1)
                true_projected_points = true_projected_points[to_update].unsqueeze(1)
                visibility_mask = visibility_mask[to_update].unsqueeze(1)
            error = self._calculate_error(u, v, true_projected_points, visibility_mask, self._error_scale)
            if self._error is not None and self._error_mask is not None:
                self._error = self._error.masked_scatter(to_update, error)
            else:
                self._error = error
            self._error_mask = None
        return self._error

    def get_gradient(self) -> torch.Tensor:
        """Get the gradient for each estimate, BxExP"""
        if self._gradient is None:
            world_points = self.get_world_points()
            u = self.get_u()
            v = self.get_v()
            camera_relative_points = self.get_camera_relative_points()
            rotation_gradients = self._orientation.vector_gradient()
            if not self._enable_grad_gradients:
                camera_relative_points = camera_relative_points.detach()
                u = u.detach()
                v = v.detach()
                world_points = world_points.detach()
                rotation_gradients.detach()
            orientation_gradients = self._orientation.parameter_gradient(
                world_points[:, :, None, :, :]
            )

            residuals_u = (
                self._error_scale * self._visibility_mask[:, None, :, :]
                * (u - self._true_projected_points[:, None, :, :, 0]).sign()
            )
            residuals_v = (
                self._error_scale * self._visibility_mask[:, None, :, :]
                * (v - self._true_projected_points[:, None, :, :, 1]).sign()
            )

            partial_derivatives = _compute_gradient_from_intermediates(
                x_prime=camera_relative_points[:, :, :, :, 0],
                y_prime=camera_relative_points[:, :, :, :, 1],
                z_prime=camera_relative_points[:, :, :, :, 2],
                focal_length=self._focal_length,
                orientation_gradients=orientation_gradients,
                rotated_vector_gradients=rotation_gradients,
                max_gradient=self._max_gradient,
            )
            gradient = _stack_gradients(
                residuals_u, residuals_v, partial_derivatives
            )
            self._gradient = gradient
            self._gradient_mask = None
        elif self._gradient_mask is not None:
            to_update = torch.logical_not(self._gradient_mask)
            camera_relative_points = self._get_camera_relative_points()
            camera_relative_points = camera_relative_points[to_update]
            camera_relative_points = camera_relative_points.unsqueeze(1)
            u = self._get_u()
            visibility_mask = self._visibility_mask[:, None, :, :].expand_as(u)
            u = u[to_update]
            u = u.unsqueeze(1)
            v = self._get_v()
            v = v[to_update]
            v = v.unsqueeze(1)
            visibility_mask = visibility_mask[to_update].unsqueeze(1)
            focal_length = self._focal_length[to_update]
            focal_length = focal_length.unsqueeze(1)
            true_projected_points = self._true_projected_points.unsqueeze(1).tile(
                1, self.num_estimates, 1, 1, 1
            )
            true_projected_points = true_projected_points[to_update]
            true_projected_points = true_projected_points.unsqueeze(1)
            world_points = self._get_world_points()
            world_points = world_points[to_update]
            world_points = world_points.unsqueeze(1)
            orientation = self._orientation.slice(to_update)
            orientation_gradients = orientation.parameter_gradient(
                world_points[:, :, None, :, :]
            )
            rotation_gradients = orientation.vector_gradient()
            residuals_u = (
                self._error_scale * visibility_mask * (u - true_projected_points[:, :, :, :, 0]).sign()
            )
            residuals_v = (
                self._error_scale * visibility_mask * (v - true_projected_points[:, :, :, :, 1]).sign()
            )
            partial_derivatives = _compute_gradient_from_intermediates(
                x_prime=camera_relative_points[:, :, :, :, 0],
                y_prime=camera_relative_points[:, :, :, :, 1],
                z_prime=camera_relative_points[:, :, :, :, 2],
                focal_length=focal_length,
                orientation_gradients=orientation_gradients,
                rotated_vector_gradients=rotation_gradients,
                max_gradient=self._max_gradient,
            )
            gradient = _stack_gradients(residuals_u, residuals_v, partial_derivatives)
            self._gradient = self._gradient.masked_scatter(
                to_update.unsqueeze(-1), gradient
            )
            self._gradient_mask = None
        return self._gradient

    def as_parameters_vector(self) -> torch.Tensor:
        orientation_vector = self._orientation.as_parameters_vector()
        return torch.cat(
            [
                self.cx,
                self.cy,
                self.focal_length,
                # Stack the camera properties, by parameter
                orientation_vector[:, :, :, 0],
                orientation_vector[:, :, :, 1],
                orientation_vector[:, :, :, 2],
                self._translation[:, :, :, 0],
                self._translation[:, :, :, 1],
                self._translation[:, :, :, 1],
                # Stack the world points, per axis
                self._world_points[:, :, :, 0],
                self._world_points[:, :, :, 1],
                self._world_points[:, :, :, 2],
            ],
            dim=-1,
        )

    def add(self, parameters: torch.Tensor) -> Self:
        # Find the slice indices for the parameters, based on the number of views and world points
        a_idx = self.VIEW_START
        b_idx = a_idx + self._num_views
        c_idx = b_idx + self._num_views
        tx_idx = c_idx + self._num_views
        ty_idx = tx_idx + self._num_views
        tz_idx = ty_idx + self._num_views
        x_idx = tz_idx + self._num_views
        y_idx = x_idx + self._num_points - 2
        z_idx = y_idx + self._num_points - 2
        end_idx = z_idx + self._num_points - 3
        a_params = parameters[:, :, a_idx:b_idx]
        b_params = parameters[:, :, b_idx:c_idx]
        c_params = parameters[:, :, c_idx:tx_idx]
        tx_params = parameters[:, :, tx_idx:ty_idx]
        ty_params = parameters[:, :, ty_idx:tz_idx]
        tz_params = parameters[:, :, tz_idx:x_idx]
        x_params = parameters[:, :, x_idx:y_idx]
        y_params = parameters[:, :, y_idx:z_idx]
        z_params = parameters[:, :, z_idx:end_idx]

        t_params = torch.stack([tx_params, ty_params, tz_params], dim=-1)
        z_params = torch.cat([torch.zeros_like(z_params[:, :, 0:1]), z_params], dim=-1)
        point_params = torch.stack([x_params, y_params, z_params], dim=-1)
        new_orientation = self._orientation.add_lie_parameters(
            torch.stack([a_params, b_params, c_params], dim=-1).unsqueeze(-2),
            constrain=self._constrain,
        )

        new_focal_length = self._focal_length + parameters[:, :, self.F]
        new_cx = self._cx + parameters[:, :, self.CX]
        new_cy = self._cy + parameters[:, :, self.CY]
        if self._constrain:
            # Apply constraints to the camera parameters
            new_focal_length = new_focal_length.clamp(min=self.maximum_pixel_ratio, max=1e3)
            new_cx = new_cx.clamp(min=-1.0, max=1.0)
            new_cy = new_cy.clamp(min=-1.0, max=1.0)
        return type(self)(
            focal_length=new_focal_length,
            cx=new_cx,
            cy=new_cy,
            translation=self._translation + t_params,
            orientation=new_orientation,
            world_points=self._world_points + point_params,
            true_projected_points=self._true_projected_points,
            visibility_mask=self._visibility_mask,
            minimum_z_distance=self.minimum_z_distance,
            constrain=self._constrain,
            max_gradient=self._max_gradient,
            enable_error_gradients=self._enable_error_gradients,
            enable_grad_gradients=self._enable_grad_gradients,
        )

    def masked_update(self, other: Self, mask: torch.Tensor) -> Self:
        if other._true_projected_points is not self._true_projected_points:
            raise ValueError(
                f"Can only do masked update between instances targeting the same points"
            )
        focal_length = torch.where(mask, other._focal_length, self._focal_length)
        cx = torch.where(mask, other._cx, self._cx)
        cy = torch.where(mask, other._cy, self._cy)
        vector_mask = mask[:, :, None, None].tile(
            1, 1, self._translation.size(2), self._translation.size(3)
        )
        orientation = self._orientation.masked_update(
            other._orientation, vector_mask.unsqueeze(-2)
        )
        translation = torch.where(vector_mask, other._translation, self._translation)
        if other._world_points is self._world_points:
            # Simple shorthand equality check. If they happen to be the same tensor, we can just reuse it.
            # Should happen most of the time.
            world_points = self._world_points
        else:
            # TODO: It might be more efficient to use expand_as
            world_mask = mask[:, :, None, None].tile(
                1, 1, *self._world_points.shape[2:]
            )
            world_points = torch.where(
                world_mask, other._world_points, self._world_points
            )
        error, error_mask = masked_merge_tensors(
            self._error, self._error_mask, other._error, other._error_mask, mask
        )
        gradient, gradient_mask = masked_merge_tensors(
            self._gradient,
            self._gradient_mask,
            other._gradient,
            other._gradient_mask,
            mask,
        )
        return type(self)(
            focal_length=focal_length,
            cx=cx,
            cy=cy,
            translation=translation,
            orientation=orientation,
            world_points=world_points,
            true_projected_points=self._true_projected_points,
            visibility_mask=self._visibility_mask,
            minimum_z_distance=self.minimum_z_distance,
            constrain=self._constrain,
            max_gradient=self._max_gradient,
            enable_error_gradients=self._enable_error_gradients,
            enable_grad_gradients=self._enable_grad_gradients,
            _error=error,
            _error_mask=error_mask,
            _gradient=gradient,
            _gradient_mask=gradient_mask,
        )

    def get_world_points(self) -> torch.Tensor:
        """
        Get the world points.
        Performs no normalisation by default, leading to the model being over-parameterised.
        Subclasses could override this to remove 6 degrees of freedom, in whatever manner.
        :return:
        """
        return self._world_points

    def get_camera_relative_points(self) -> torch.Tensor:
        """
        :returns: 3d points relative to each camera, shape BxExMxNx3
        """
        if self._camera_relative_points is None:
            self._camera_relative_points = self._calculate_camera_relative_points()
        return self._camera_relative_points

    def _calculate_camera_relative_points(self) -> torch.Tensor:
        """
        Calculate the points relative to each viewpoint.
        Does not perform any clamping or constraint on the Z axis.
        A subclass might want to override this to do that.
        """
        world_points = self.get_world_points()
        # Rotate the points by the camera rotations.
        rotated_points = self._orientation.rotate_vector(
            world_points[:, :, None, :, :]
        )
        rotated_points = rotated_points + self._translation[:, :, :, None, :]
        return rotated_points

    def get_u(self) -> torch.Tensor:
        """
        :returns: BxExMxN
        """

        if self._u is None:
            camera_relative_points = self.get_camera_relative_points()
            self._u = self.calculate_projection(
                focal_length=self.focal_length.unsqueeze(-1).unsqueeze(-1),
                lateral_axis=camera_relative_points[:, :, :, :, 0],
                forward_axis=camera_relative_points[:, :, :, :, 2],
                principal_point=self.cx.unsqueeze(-1).unsqueeze(-1),
            )
        return self._u

    def get_v(self) -> torch.Tensor:
        """
        :returns: BxExMxN
        """
        if self._v is None:
            camera_relative_points = self.get_camera_relative_points()
            self._v = self.calculate_projection(
                focal_length=self.focal_length.unsqueeze(-1).unsqueeze(-1),
                lateral_axis=camera_relative_points[:, :, :, :, 1],
                forward_axis=camera_relative_points[:, :, :, :, 2],
                principal_point=self.cy.unsqueeze(-1).unsqueeze(-1),
            )
        return self._v

    @staticmethod
    def calculate_projection(focal_length: torch.Tensor, lateral_axis: torch.Tensor, forward_axis: torch.Tensor, principal_point: torch.Tensor):
        """
        Given a focal length, x, z, and cx, compute f * x / z + cx
        This is the same for x and y.
        A subclass might override this to provide clamping or change the boundary behaviour, particularly as z -> 0.

        All input tensors should be broadcast-able.
        """
        return focal_length * lateral_axis / forward_axis + principal_point

    @staticmethod
    def _calculate_error(u: torch.Tensor, v: torch.Tensor, true_projected_points: torch.Tensor, visibility_mask: torch.Tensor, error_scale: torch.Tensor) -> torch.Tensor:
        """
        Calculate the error between the true and estimated points.
        Defaults to the L1 error, override this to change
        """
        u_residuals = u - true_projected_points[:, None, :, :, 0]
        v_residuals = v - true_projected_points[:, None, :, :, 1]
        u_residuals = u_residuals * visibility_mask[:, None, :, :]
        v_residuals = v_residuals * visibility_mask[:, None, :, :]
        return (error_scale * u_residuals.abs()).sum(
            dim=(-2, -1)
        ) + (error_scale * v_residuals.square()).sum(dim=(-2, -1))

    @staticmethod
    def _calculate_gradient():
        # CX/CY derivatives are 1 for their coordinate, and 0 for the other
        cx_gradients = residuals_u.sum(dim=(-2, -1)).unsqueeze(-1)
        cy_gradients = residuals_v.sum(dim=(-2, -1)).unsqueeze(-1)
        # FX derivatives are summed over both the views and the points
        f_gradients = (
                (residuals_u * partial_derivatives.partial_du_df).sum(dim=(-2, -1))
                + (residuals_v * partial_derivatives.partial_dv_df).sum(dim=(-2, -1))
        ).unsqueeze(-1)
        # Compute camera view gradients. Summed over all world points
        a_gradients = (residuals_u * partial_derivatives.partial_du_da).sum(dim=-1) + (
                residuals_v * partial_derivatives.partial_dv_da
        ).sum(dim=-1)
        b_gradients = (residuals_u * partial_derivatives.partial_du_db).sum(dim=-1) + (
                residuals_v * partial_derivatives.partial_dv_db
        ).sum(dim=-1)
        c_gradients = (residuals_u * partial_derivatives.partial_du_dc).sum(dim=-1) + (
                residuals_v * partial_derivatives.partial_dv_dc
        ).sum(dim=-1)
        tx_gradients = (residuals_u * partial_derivatives.partial_du_dtx).sum(dim=-1)
        ty_gradients = (residuals_v * partial_derivatives.partial_dv_dty).sum(dim=-1)
        tz_gradients = (residuals_u * partial_derivatives.partial_du_dtz).sum(dim=-1) + (
                residuals_v * partial_derivatives.partial_dv_dtz
        ).sum(dim=-1)
        # Compute world point gradients, summed over all views
        world_x_gradients = (
                                    residuals_u[:, :, :, 2:] * partial_derivatives.partial_du_dx[:, :, :, 2:]
                            ).sum(dim=-2) + (
                                    residuals_v[:, :, :, 2:] * partial_derivatives.partial_dv_dx[:, :, :, 2:]
                            ).sum(
            dim=-2
        )
        world_y_gradients = (
                residuals_u[:, :, :, 2:] * partial_derivatives.partial_du_dy[:, :, :, 2:]
                + residuals_v[:, :, :, 2:] * partial_derivatives.partial_dv_dy[:, :, :, 2:]
        ).sum(dim=-2)
        world_z_gradients = (
                residuals_u[:, :, :, 3:] * partial_derivatives.partial_du_dz[:, :, :, 3:]
                + residuals_v[:, :, :, 3:] * partial_derivatives.partial_dv_dz[:, :, :, 3:]
        ).sum(dim=-2)
        return torch.cat(
            [
                cx_gradients,
                cy_gradients,
                f_gradients,
                # Stack the camera gradients, by parameter
                a_gradients,
                b_gradients,
                c_gradients,
                tx_gradients,
                ty_gradients,
                tz_gradients,
                # Stack the world point gradients, per axis
                world_x_gradients,
                world_y_gradients,
                world_z_gradients,
            ],
            dim=-1,
        )

    @staticmethod
    def _calculate_residuals(u: torch.Tensor, v: torch.Tensor, true_projected_points: torch.Tensor, visibility_mask: torch.Tensor, error_scale: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Calculate the gradient of the error with respect to the predicted points.
        Return de/du and de/dv.
        Used to calculate the gradients. If `_calculate_error` is overridden, this should be too.
        """
        residuals_u = (
                error_scale * visibility_mask[:, None, :, :]
                * (u - true_projected_points[:, None, :, :, 0]).sign()
        )
        residuals_v = (
                error_scale * visibility_mask[:, None, :, :]
                * (v - true_projected_points[:, None, :, :, 1]).sign()
        )
        return residuals_u, residuals_v

    @classmethod
    def _calculate_projected_point_gradients(
            cls,
            x_prime: torch.Tensor,
            y_prime: torch.Tensor,
            z_prime: torch.Tensor,
            focal_length: torch.Tensor,
            orientation_gradients: torch.Tensor,
            rotated_vector_gradients: torch.Tensor,
    ) -> _CameraGradients:
        """
        Compute the gradient of u and v with respect to each of the parameters in the model.
        :param x_prime: BxExMxN
        :param y_prime: BxExMxN
        :param z_prime: BxExMxN, assumed strictly positive
        :param focal_length: BxE
        :param orientation_gradients: BxExMxNx3x3
        :param rotated_vector_gradients: BxExMx3x3
        :return: A NamedTuple, with all the partial derivatives
        """
        while focal_length.ndim < z_prime.ndim:
            focal_length = focal_length.unsqueeze(-1)
        # The ratio f/z must be constrained prior to calculation,
        # since z may be small and f may be large.
        # The gradient w.r.t. z is -f/z^2, which can grow out of proportion.
        # z on its own should not be less than 1e-3, which caps the gradient for
        # inv_z_prime at -1e6.
        # We also choose a scale factor to try and uniformly scale the gradient vector.
        inv_z_prime = 1.0 / z_prime
        f_on_z_prime = focal_length * inv_z_prime
        x_on_z_prime = x_prime * inv_z_prime
        y_on_z_prime = y_prime * inv_z_prime
        du_dxprime = f_on_z_prime
        dv_dyprime = f_on_z_prime
        du_dzprime = -1.0 * f_on_z_prime * x_on_z_prime  # = -f x / z^2
        dv_dzprime = -1.0 * f_on_z_prime * y_on_z_prime

        # Camera parameter derivatives are fairly simple, cx and cy are ones/zeros
        partial_du_df = x_on_z_prime
        partial_dv_df = y_on_z_prime

        # Translation parameters are also fairly simple
        # Note that ty does not affect u and tx does not affect v
        partial_du_dtx = (scale_factor * du_dxprime).clip(min=-max_gradient, max=max_gradient)
        partial_dv_dty = (scale_factor * dv_dyprime).clip(min=-max_gradient, max=max_gradient)
        partial_du_dtz = (scale_factor * du_dzprime).clip(min=-max_gradient, max=max_gradient)
        partial_dv_dtz = (scale_factor * dv_dzprime).clip(min=-max_gradient, max=max_gradient)

        # The rotation derivatives
        partial_du_da = (scale_factor * (
                du_dxprime * orientation_gradients[:, :, :, :, 0, 0]
                + du_dzprime * orientation_gradients[:, :, :, :, 2, 0]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_du_db = (scale_factor * (
                du_dxprime * orientation_gradients[:, :, :, :, 0, 1]
                + du_dzprime * orientation_gradients[:, :, :, :, 2, 1]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_du_dc = (scale_factor * (
                du_dxprime * orientation_gradients[:, :, :, :, 0, 2]
                + du_dzprime * orientation_gradients[:, :, :, :, 2, 2]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_dv_da = (scale_factor * (
                dv_dyprime * orientation_gradients[:, :, :, :, 1, 0]
                + dv_dzprime * orientation_gradients[:, :, :, :, 2, 0]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_dv_db = (scale_factor * (
                dv_dyprime * orientation_gradients[:, :, :, :, 1, 1]
                + dv_dzprime * orientation_gradients[:, :, :, :, 2, 1]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_dv_dc = (scale_factor * (
                dv_dyprime * orientation_gradients[:, :, :, :, 1, 2]
                + dv_dzprime * orientation_gradients[:, :, :, :, 2, 2]
        )).clip(min=-max_gradient, max=max_gradient)

        # Lastly, the gradients for the world points
        # These are again chain rule patterns
        partial_du_dx = (scale_factor * (
                du_dxprime * rotated_vector_gradients[:, :, :, :, 0, 0]
                + du_dzprime * rotated_vector_gradients[:, :, :, :, 2, 0]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_dv_dx = (scale_factor * (
                dv_dyprime * rotated_vector_gradients[:, :, :, :, 1, 0]
                + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 0]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_du_dy = (scale_factor * (
                du_dxprime * rotated_vector_gradients[:, :, :, :, 0, 1]
                + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 1]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_dv_dy = (scale_factor * (
                dv_dyprime * rotated_vector_gradients[:, :, :, :, 1, 1]
                + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 1]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_du_dz = (scale_factor * (
                du_dxprime * rotated_vector_gradients[:, :, :, :, 0, 2]
                + du_dzprime * rotated_vector_gradients[:, :, :, :, 2, 2]
        )).clip(min=-max_gradient, max=max_gradient)
        partial_dv_dz = (scale_factor * (
                dv_dyprime * rotated_vector_gradients[:, :, :, :, 1, 2]
                + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 2]
        )).clip(min=-max_gradient, max=max_gradient)

        return cls._CameraGradients(
            partial_du_df=partial_du_df,
            partial_dv_df=partial_dv_df,
            partial_du_da=partial_du_da,
            partial_dv_da=partial_dv_da,
            partial_du_db=partial_du_db,
            partial_dv_db=partial_dv_db,
            partial_du_dc=partial_du_dc,
            partial_dv_dc=partial_dv_dc,
            partial_du_dtx=partial_du_dtx,
            partial_dv_dty=partial_dv_dty,
            partial_du_dtz=partial_du_dtz,
            partial_dv_dtz=partial_dv_dtz,
            partial_du_dx=partial_du_dx,
            partial_dv_dx=partial_dv_dx,
            partial_du_dy=partial_du_dy,
            partial_dv_dy=partial_dv_dy,
            partial_du_dz=partial_du_dz,
            partial_dv_dz=partial_dv_dz,
        )

