from typing import Final, NamedTuple, Self
import torch

from deep_attention_visual_odometry.solvers import IOptimisableFunction
from deep_attention_visual_odometry.utils import masked_merge_tensors
from deep_attention_visual_odometry.geometry.lie_rotation import LieRotation


class PinholeCameraModelLeastSquares(IOptimisableFunction):
    """
    Given a set of points across multiple views,
    jointly optimise for the 3D positions of the points, the camera intrinsics, and extrinsics.

    Based on
    "More accurate pinhole camera calibration with imperfect planar target" by
    Klaus H Strobl and Gerd Hirzinger in ICCV 2011

    A simple camera model, projecting 3D world points to produce image coordinates in multiple views.
    Supports a batch dimension and multiple parallel estimates,
    so the image coordinates should be shape BxExMxNx2.

    Where B is batch size, E the number of estimates, M the number of camera views,
    and N the number of points
    """

    # Parameter indices in order
    CX: Final[int] = 0
    CY: Final[int] = 1
    F: Final[int] = 2
    VIEW_START: Final[int] = 3

    def __init__(
        self,
        focal_length: torch.Tensor,
        cx: torch.Tensor,
        cy: torch.Tensor,
        translation: torch.Tensor,
        orientation: LieRotation,
        world_points: torch.Tensor,
        true_projected_points: torch.Tensor,
        visibility_mask: torch.Tensor,
        minimum_distance: float = 1e-3,
        maximum_pixel_ratio: float = 5.0,
        constrain: bool = False,
        max_gradient: float = -1.0,
        _error: torch.Tensor | None = None,
        _gradient: torch.Tensor | None = None,
        _error_mask: torch.Tensor | None = None,
        _gradient_mask: torch.Tensor | None = None,
    ):
        """
        :param focal_length: BxE (assumed fixed for all views)
        :param cx: BxE (fixed for all views)
        :param cy: BxE (fixed for all views)
        :param translation: BxExMx3
        :param orientation: BxExMx3
        :param world_points: BxEx(N - 2)x3 (fixed for all views)
        :param true_projected_points: BxMxNx2
        :param visibility_mask: BxMxN Whether or not a given point appears within a view.
        Missing points have their error ignored.
        :param minimum_distance: Minimum distance from the camera plane
        :param maximum_pixel_ratio: The maximum ratio for x/z and y/z.
        Since we expect the image coordinates to range [-1, 1], constraining this keeps the error sane.
        :param _error: Pre-calculated error values (generated by parent)
        :param _gradient: Pre-calculated gradient values (generated by parent)
        :param _error_mask: Where false, the error value needs to be re-calculated (used by masked_update)
        :param _gradient_mask: Where false, the gradient needs to be re-calculated (used by masked_update)
        """
        self.minimum_distance = float(minimum_distance)
        self.maximum_pixel_ratio = 1.0 / abs(float(maximum_pixel_ratio))
        self._constrain = bool(constrain)
        self._max_gradient = float(max_gradient)
        self._num_views = true_projected_points.size(1)
        self._num_points = true_projected_points.size(2)
        self._num_estimates = focal_length.size(1)
        self._error_scale = torch.tensor(
            1.0 / (self._num_views * self._num_points)
        ).sqrt()
        self._focal_length = focal_length
        self._cx = cx
        self._cy = cy
        self._translation = translation
        self._world_points = world_points
        self._true_projected_points = true_projected_points
        self._visibility_mask = visibility_mask
        self._orientation = orientation
        self._camera_relative_points = None
        self._u = None
        self._v = None
        self._error = _error
        self._gradient = _gradient

        self._error_mask = _error_mask
        self._gradient_mask = _gradient_mask

    @property
    def batch_size(self) -> int:
        return self._true_projected_points.size(0)

    @property
    def num_estimates(self) -> int:
        return self._num_estimates

    @property
    def num_parameters(self) -> int:
        return 3 + 6 * self._num_views + 3 * self._num_points - 7

    @property
    def device(self) -> torch.device:
        return self._true_projected_points.device

    @property
    def focal_length(self) -> torch.Tensor:
        return self._focal_length

    @property
    def cx(self) -> torch.Tensor:
        return self._cx

    @property
    def cy(self) -> torch.Tensor:
        return self._cy

    def get_error(self) -> torch.Tensor:
        """
        :returns: Total square error for each estimate, BxE.
        """
        if self._error is None:
            u = self._get_u()
            v = self._get_v()
            u_residuals = u - self._true_projected_points[:, None, :, :, 0]
            v_residuals = v - self._true_projected_points[:, None, :, :, 1]
            u_residuals = u_residuals * self._visibility_mask[:, None, :, :]
            v_residuals = v_residuals * self._visibility_mask[:, None, :, :]
            self._error = (self._error_scale * u_residuals.square()).sum(
                dim=(-2, -1)
            ) + (self._error_scale * v_residuals.square()).sum(dim=(-2, -1))
            self._error_mask = None
        elif self._error_mask is not None:
            u = self._get_u()
            v = self._get_v()
            visibility_mask = self._visibility_mask[:, None, :, :].expand_as(u)
            to_update = torch.logical_not(self._error_mask)
            u = u[to_update]
            v = v[to_update]
            visibility_mask = visibility_mask[to_update]
            true_projected_points = self._true_projected_points.unsqueeze(1).tile(
                1, self.num_estimates, 1, 1, 1
            )
            true_projected_points = true_projected_points[to_update]

            u_residuals = u - true_projected_points[:, :, :, 0]
            v_residuals = v - true_projected_points[:, :, :, 1]
            u_residuals = u_residuals * visibility_mask
            v_residuals = v_residuals * visibility_mask
            new_error = (
                self._error_scale * u_residuals.square()
            ).sum(dim=(-2, -1)) + (
                self._error_scale * v_residuals.square()
            ).sum(
                dim=(-2, -1)
            )
            self._error = self._error.masked_scatter(to_update, new_error)
            self._error_mask = None
        return self._error

    def get_gradient(self) -> torch.Tensor:
        """Get the gradient for each estimate, BxExP"""
        if self._gradient is None:
            world_points = self._get_world_points()
            orientation_gradients = self._orientation.parameter_gradient(
                world_points[:, :, None, :, :]
            )
            rotation_gradients = self._orientation.vector_gradient()
            camera_relative_points = self._get_camera_relative_points()
            u = self._get_u()
            v = self._get_v()
            residuals_u = (
                2.0
                * self._error_scale * self._visibility_mask[:, None, :, :]
                * (u - self._true_projected_points[:, None, :, :, 0])
            )
            residuals_v = (
                2.0
                * self._error_scale * self._visibility_mask[:, None, :, :]
                * (v - self._true_projected_points[:, None, :, :, 1])
            )

            partial_derivatives = _compute_gradient_from_intermediates(
                x_prime=camera_relative_points[:, :, :, :, 0],
                y_prime=camera_relative_points[:, :, :, :, 1],
                z_prime=camera_relative_points[:, :, :, :, 2],
                focal_length=self._focal_length,
                orientation_gradients=orientation_gradients,
                rotated_vector_gradients=rotation_gradients,
                max_gradient=self._max_gradient,
            )
            gradient = _stack_gradients(
                residuals_u, residuals_v, partial_derivatives
            )
            self._gradient = gradient
            self._gradient_mask = None
        elif self._gradient_mask is not None:
            to_update = torch.logical_not(self._gradient_mask)
            camera_relative_points = self._get_camera_relative_points()
            camera_relative_points = camera_relative_points[to_update]
            camera_relative_points = camera_relative_points.unsqueeze(1)
            u = self._get_u()
            visibility_mask = self._visibility_mask[:, None, :, :].expand_as(u)
            u = u[to_update]
            u = u.unsqueeze(1)
            v = self._get_v()
            v = v[to_update]
            v = v.unsqueeze(1)
            visibility_mask = visibility_mask[to_update].unsqueeze(1)
            focal_length = self._focal_length[to_update]
            focal_length = focal_length.unsqueeze(1)
            true_projected_points = self._true_projected_points.unsqueeze(1).tile(
                1, self.num_estimates, 1, 1, 1
            )
            true_projected_points = true_projected_points[to_update]
            true_projected_points = true_projected_points.unsqueeze(1)
            world_points = self._get_world_points()
            world_points = world_points[to_update]
            world_points = world_points.unsqueeze(1)
            orientation = self._orientation.slice(to_update)
            orientation_gradients = orientation.parameter_gradient(
                world_points[:, :, None, :, :]
            )
            rotation_gradients = orientation.vector_gradient()
            residuals_u = (
                2.0 * self._error_scale * visibility_mask * (u - true_projected_points[:, :, :, :, 0])
            )
            residuals_v = (
                2.0 * self._error_scale * visibility_mask * (v - true_projected_points[:, :, :, :, 1])
            )
            partial_derivatives = _compute_gradient_from_intermediates(
                x_prime=camera_relative_points[:, :, :, :, 0],
                y_prime=camera_relative_points[:, :, :, :, 1],
                z_prime=camera_relative_points[:, :, :, :, 2],
                focal_length=focal_length,
                orientation_gradients=orientation_gradients,
                rotated_vector_gradients=rotation_gradients,
                max_gradient=self._max_gradient,
            )
            gradient = _stack_gradients(residuals_u, residuals_v, partial_derivatives)
            self._gradient = self._gradient.masked_scatter(
                to_update.unsqueeze(-1), gradient
            )
            self._gradient_mask = None
        return self._gradient

    def as_parameters_vector(self) -> torch.Tensor:
        orientation_vector = self._orientation.as_parameters_vector()
        return torch.cat(
            [
                self.cx,
                self.cy,
                self.focal_length,
                # Stack the camera properties, by parameter
                orientation_vector[:, :, :, 0],
                orientation_vector[:, :, :, 1],
                orientation_vector[:, :, :, 2],
                self._translation[:, :, :, 0],
                self._translation[:, :, :, 1],
                self._translation[:, :, :, 1],
                # Stack the world points, per axis
                self._world_points[:, :, :, 0],
                self._world_points[:, :, :, 1],
                self._world_points[:, :, :, 2],
            ],
            dim=-1,
        )

    def add(self, parameters: torch.Tensor) -> Self:
        # Find the slice indices for the parameters, based on the number of views and world points
        a_idx = self.VIEW_START
        b_idx = a_idx + self._num_views
        c_idx = b_idx + self._num_views
        tx_idx = c_idx + self._num_views
        ty_idx = tx_idx + self._num_views
        tz_idx = ty_idx + self._num_views
        x_idx = tz_idx + self._num_views
        y_idx = x_idx + self._num_points - 2
        z_idx = y_idx + self._num_points - 2
        end_idx = z_idx + self._num_points - 3
        a_params = parameters[:, :, a_idx:b_idx]
        b_params = parameters[:, :, b_idx:c_idx]
        c_params = parameters[:, :, c_idx:tx_idx]
        tx_params = parameters[:, :, tx_idx:ty_idx]
        ty_params = parameters[:, :, ty_idx:tz_idx]
        tz_params = parameters[:, :, tz_idx:x_idx]
        x_params = parameters[:, :, x_idx:y_idx]
        y_params = parameters[:, :, y_idx:z_idx]
        z_params = parameters[:, :, z_idx:end_idx]

        t_params = torch.stack([tx_params, ty_params, tz_params], dim=-1)
        z_params = torch.cat([torch.zeros_like(z_params[:, :, 0:1]), z_params], dim=-1)
        point_params = torch.stack([x_params, y_params, z_params], dim=-1)
        new_orientation = self._orientation.add_lie_parameters(
            torch.stack([a_params, b_params, c_params], dim=-1).unsqueeze(-2),
            constrain=self._constrain,
        )

        new_focal_length = self._focal_length + parameters[:, :, self.F]
        new_cx = self._cx + parameters[:, :, self.CX]
        new_cy = self._cy + parameters[:, :, self.CY]
        if self._constrain:
            # Apply constraints to the camera parameters
            new_focal_length = new_focal_length.clamp(min=self.maximum_pixel_ratio, max=1e3)
            new_cx = new_cx.clamp(min=-1.0, max=1.0)
            new_cy = new_cy.clamp(min=-1.0, max=1.0)
        return type(self)(
            focal_length=new_focal_length,
            cx=new_cx,
            cy=new_cy,
            translation=self._translation + t_params,
            orientation=new_orientation,
            world_points=self._world_points + point_params,
            true_projected_points=self._true_projected_points,
            visibility_mask=self._visibility_mask,
            minimum_distance=self.minimum_distance,
            constrain=self._constrain,
            max_gradient=self._max_gradient
        )

    def masked_update(self, other: Self, mask: torch.Tensor) -> Self:
        if other._true_projected_points is not self._true_projected_points:
            raise ValueError(
                f"Can only do masked update between instances targeting the same points"
            )
        focal_length = torch.where(mask, other._focal_length, self._focal_length)
        cx = torch.where(mask, other._cx, self._cx)
        cy = torch.where(mask, other._cy, self._cy)
        vector_mask = mask[:, :, None, None].tile(
            1, 1, self._translation.size(2), self._translation.size(3)
        )
        orientation = self._orientation.masked_update(
            other._orientation, vector_mask.unsqueeze(-2)
        )
        translation = torch.where(vector_mask, other._translation, self._translation)
        if other._world_points is self._world_points:
            # Simple shorthand equality check. If they happen to be the same tensor, we can just reuse it.
            # Should happen most of the time.
            world_points = self._world_points
        else:
            # TODO: It might be more efficient to use expand_as
            world_mask = mask[:, :, None, None].tile(
                1, 1, *self._world_points.shape[2:]
            )
            world_points = torch.where(
                world_mask, other._world_points, self._world_points
            )
        error, error_mask = masked_merge_tensors(
            self._error, self._error_mask, other._error, other._error_mask, mask
        )
        gradient, gradient_mask = masked_merge_tensors(
            self._gradient,
            self._gradient_mask,
            other._gradient,
            other._gradient_mask,
            mask,
        )
        return type(self)(
            focal_length=focal_length,
            cx=cx,
            cy=cy,
            translation=translation,
            orientation=orientation,
            world_points=world_points,
            true_projected_points=self._true_projected_points,
            visibility_mask=self._visibility_mask,
            minimum_distance=self.minimum_distance,
            constrain=self._constrain,
            max_gradient=self._max_gradient,
            _error=error,
            _error_mask=error_mask,
            _gradient=gradient,
            _gradient_mask=gradient_mask,
        )

    def _get_world_points(self) -> torch.Tensor:
        """
        Get the world points, augmented with two additional points.
        We constrain the world points, such that the first point is [0,0,0],
        the second point is [1, 0, 0] (defining the x axis), and the third point is [x, y, 0],
        (defining the xy plane).
        :return:
        """
        first_two_points = torch.zeros(
            self.batch_size,
            self.num_estimates,
            2,
            3,
            device=self._world_points.device,
            dtype=self._world_points.dtype,
        )
        first_two_points[:, :, 1, 0] = 1.0
        third_point = torch.cat(
            [
                self._world_points[:, :, 0:1, 0:2],
                torch.zeros_like(self._world_points[:, :, 0:1, 2:3]),
            ],
            dim=-1,
        )
        world_points = torch.cat(
            [first_two_points, third_point, self._world_points[:, :, 1:, :]], dim=2
        )
        return world_points

    def _get_camera_relative_points(self) -> torch.Tensor:
        """
        :returns: 3d points relative to each camera, shape BxExMxNx3
        """
        if self._camera_relative_points is None:
            world_points = self._get_world_points()
            # Rotate the points by the camera rotations.
            rotated_points = self._orientation.rotate_vector(
                world_points[:, :, None, :, :]
            )
            rotated_points = rotated_points + self._translation[:, :, :, None, :]
            # Clamp the camera-relative z' to treat all points as "in front" of the camera,
            # Due to the division, the optmisation cannot cross through Z' = 0,
            # because the projected points go to infinity, and thus so does the error.
            # It is a relatively safe assumption that any point we can see, is in front of the camera.
            min_z = (
                (self.maximum_pixel_ratio * rotated_points[:, :, :, :, 0:2])
                .abs()
                .max(dim=-1)
                .values
            )
            min_z = torch.clamp(min_z, min=self.minimum_distance)
            rotated_points = torch.cat(
                [
                    rotated_points[:, :, :, :, 0:2],
                    torch.maximum(rotated_points[:, :, :, :, 2:3], min_z.unsqueeze(-1)),
                ],
                dim=-1,
            )
            self._camera_relative_points = rotated_points
        return self._camera_relative_points

    def _get_u(self) -> torch.Tensor:
        """
        :returns: BxExMxN
        """

        if self._u is None:
            camera_relative_points = self._get_camera_relative_points()
            self._u = self._focal_length.view(
                *self._focal_length.shape, 1, 1
            ) * camera_relative_points[:, :, :, :, 0] / camera_relative_points[
                :, :, :, :, 2
            ] + self._cx.view(
                *self._cx.shape, 1, 1
            )
        return self._u

    def _get_v(self) -> torch.Tensor:
        """
        :returns: BxExMxN
        """
        if self._v is None:
            camera_relative_points = self._get_camera_relative_points()
            self._v = self._focal_length.view(
                *self._focal_length.shape, 1, 1
            ) * camera_relative_points[:, :, :, :, 1] / camera_relative_points[
                :, :, :, :, 2
            ] + self._cy.view(
                *self._cy.shape, 1, 1
            )
        return self._v


class _CameraGradients(NamedTuple):
    partial_du_df: torch.Tensor
    partial_dv_df: torch.Tensor
    partial_du_da: torch.Tensor
    partial_dv_da: torch.Tensor
    partial_du_db: torch.Tensor
    partial_dv_db: torch.Tensor
    partial_du_dc: torch.Tensor
    partial_dv_dc: torch.Tensor
    partial_du_dtx: torch.Tensor
    partial_dv_dty: torch.Tensor
    partial_du_dtz: torch.Tensor
    partial_dv_dtz: torch.Tensor
    partial_du_dx: torch.Tensor
    partial_dv_dx: torch.Tensor
    partial_du_dy: torch.Tensor
    partial_dv_dy: torch.Tensor
    partial_du_dz: torch.Tensor
    partial_dv_dz: torch.Tensor


def _compute_gradient_from_intermediates(
    x_prime: torch.Tensor,
    y_prime: torch.Tensor,
    z_prime: torch.Tensor,
    focal_length: torch.Tensor,
    orientation_gradients: torch.Tensor,
    rotated_vector_gradients: torch.Tensor,
    max_gradient: float = 1e5
) -> _CameraGradients:
    """
    Compute the gradient of the error
    :param x_prime: BxExMxN
    :param y_prime: BxExMxN
    :param z_prime: BxExMxN, assumed strictly positive
    :param focal_length: BxE
    :param orientation_gradients: BxExMxNx3x3
    :param rotated_vector_gradients: BxExMx3x3
    :param max_gradient: Maximum gradient value, assumed positive
    :return:
    """
    while focal_length.ndim < z_prime.ndim:
        focal_length = focal_length.unsqueeze(-1)
    # The ratio f/z must be constrained prior to calculation,
    # since z may be small and f may be large.
    # The gradient w.r.t. z is -f/z^2, which can grow out of proportion.
    # z on its own should not be less than 1e-3, which caps the gradient for
    # inv_z_prime at -1e6.
    # We also choose a scale factor to try and uniformly scale the gradient vector.
    inv_z_prime = 1.0 / z_prime
    scale_factor = (max_gradient * inv_z_prime).clip(max=1.0)
    max_focal_length_multiple = (max_gradient / focal_length).abs()
    f_on_z_prime = focal_length * inv_z_prime.clip(min=-max_focal_length_multiple, max=max_focal_length_multiple)
    x_on_z_prime = x_prime * inv_z_prime
    y_on_z_prime = y_prime * inv_z_prime
    du_dxprime = (scale_factor * f_on_z_prime).clip(min=-max_gradient, max=max_gradient)
    dv_dyprime = (scale_factor * f_on_z_prime).clip(min=-max_gradient, max=max_gradient)
    du_dzprime = (-scale_factor * f_on_z_prime * x_on_z_prime).clip(min=-max_gradient, max=max_gradient)  # = -f x / z^2
    dv_dzprime = (-scale_factor * f_on_z_prime * y_on_z_prime).clip(min=-max_gradient, max=max_gradient)

    # Camera parameter derivatives are fairly simple, cx and cy are ones/zeros
    partial_du_df = (scale_factor * x_on_z_prime).clip(min=-max_gradient, max=max_gradient)
    partial_dv_df = (scale_factor * y_on_z_prime).clip(min=-max_gradient, max=max_gradient)

    # Translation parameters are also fairly simple
    # Note that ty does not affect u and tx does not affect v
    partial_du_dtx = (scale_factor * du_dxprime).clip(min=-max_gradient, max=max_gradient)
    partial_dv_dty = (scale_factor * dv_dyprime).clip(min=-max_gradient, max=max_gradient)
    partial_du_dtz = (scale_factor * du_dzprime).clip(min=-max_gradient, max=max_gradient)
    partial_dv_dtz = (scale_factor * dv_dzprime).clip(min=-max_gradient, max=max_gradient)

    # The rotation derivatives
    partial_du_da = (scale_factor * (
        du_dxprime * orientation_gradients[:, :, :, :, 0, 0]
        + du_dzprime * orientation_gradients[:, :, :, :, 2, 0]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_du_db = (scale_factor * (
        du_dxprime * orientation_gradients[:, :, :, :, 0, 1]
        + du_dzprime * orientation_gradients[:, :, :, :, 2, 1]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_du_dc = (scale_factor * (
        du_dxprime * orientation_gradients[:, :, :, :, 0, 2]
        + du_dzprime * orientation_gradients[:, :, :, :, 2, 2]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_dv_da = (scale_factor * (
        dv_dyprime * orientation_gradients[:, :, :, :, 1, 0]
        + dv_dzprime * orientation_gradients[:, :, :, :, 2, 0]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_dv_db = (scale_factor * (
        dv_dyprime * orientation_gradients[:, :, :, :, 1, 1]
        + dv_dzprime * orientation_gradients[:, :, :, :, 2, 1]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_dv_dc = (scale_factor * (
        dv_dyprime * orientation_gradients[:, :, :, :, 1, 2]
        + dv_dzprime * orientation_gradients[:, :, :, :, 2, 2]
    )).clip(min=-max_gradient, max=max_gradient)

    # Lastly, the gradients for the world points
    # These are again chain rule patterns
    partial_du_dx = (scale_factor * (
        du_dxprime * rotated_vector_gradients[:, :, :, :, 0, 0]
        + du_dzprime * rotated_vector_gradients[:, :, :, :, 2, 0]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_dv_dx = (scale_factor * (
        dv_dyprime * rotated_vector_gradients[:, :, :, :, 1, 0]
        + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 0]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_du_dy = (scale_factor * (
        du_dxprime * rotated_vector_gradients[:, :, :, :, 0, 1]
        + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 1]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_dv_dy = (scale_factor * (
        dv_dyprime * rotated_vector_gradients[:, :, :, :, 1, 1]
        + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 1]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_du_dz = (scale_factor * (
        du_dxprime * rotated_vector_gradients[:, :, :, :, 0, 2]
        + du_dzprime * rotated_vector_gradients[:, :, :, :, 2, 2]
    )).clip(min=-max_gradient, max=max_gradient)
    partial_dv_dz = (scale_factor * (
        dv_dyprime * rotated_vector_gradients[:, :, :, :, 1, 2]
        + dv_dzprime * rotated_vector_gradients[:, :, :, :, 2, 2]
    )).clip(min=-max_gradient, max=max_gradient)

    return _CameraGradients(
        partial_du_df=partial_du_df,
        partial_dv_df=partial_dv_df,
        partial_du_da=partial_du_da,
        partial_dv_da=partial_dv_da,
        partial_du_db=partial_du_db,
        partial_dv_db=partial_dv_db,
        partial_du_dc=partial_du_dc,
        partial_dv_dc=partial_dv_dc,
        partial_du_dtx=partial_du_dtx,
        partial_dv_dty=partial_dv_dty,
        partial_du_dtz=partial_du_dtz,
        partial_dv_dtz=partial_dv_dtz,
        partial_du_dx=partial_du_dx,
        partial_dv_dx=partial_dv_dx,
        partial_du_dy=partial_du_dy,
        partial_dv_dy=partial_dv_dy,
        partial_du_dz=partial_du_dz,
        partial_dv_dz=partial_dv_dz,
    )


def _stack_gradients(
    residuals_u: torch.Tensor,
    residuals_v: torch.Tensor,
    partial_derivatives: _CameraGradients,
) -> torch.Tensor:
    # CX/CY derivatives are 1 for their coordinate, and 0 for the other
    cx_gradients = residuals_u.sum(dim=(-2, -1)).unsqueeze(-1)
    cy_gradients = residuals_v.sum(dim=(-2, -1)).unsqueeze(-1)
    # FX derivatives are summed over both the views and the points
    f_gradients = (
        (residuals_u * partial_derivatives.partial_du_df).sum(dim=(-2, -1))
        + (residuals_v * partial_derivatives.partial_dv_df).sum(dim=(-2, -1))
    ).unsqueeze(-1)
    # Compute camera view gradients. Summed over all world points
    a_gradients = (residuals_u * partial_derivatives.partial_du_da).sum(dim=-1) + (
        residuals_v * partial_derivatives.partial_dv_da
    ).sum(dim=-1)
    b_gradients = (residuals_u * partial_derivatives.partial_du_db).sum(dim=-1) + (
        residuals_v * partial_derivatives.partial_dv_db
    ).sum(dim=-1)
    c_gradients = (residuals_u * partial_derivatives.partial_du_dc).sum(dim=-1) + (
        residuals_v * partial_derivatives.partial_dv_dc
    ).sum(dim=-1)
    tx_gradients = (residuals_u * partial_derivatives.partial_du_dtx).sum(dim=-1)
    ty_gradients = (residuals_v * partial_derivatives.partial_dv_dty).sum(dim=-1)
    tz_gradients = (residuals_u * partial_derivatives.partial_du_dtz).sum(dim=-1) + (
        residuals_v * partial_derivatives.partial_dv_dtz
    ).sum(dim=-1)
    # Compute world point gradients, summed over all views
    world_x_gradients = (
        residuals_u[:, :, :, 2:] * partial_derivatives.partial_du_dx[:, :, :, 2:]
    ).sum(dim=-2) + (
        residuals_v[:, :, :, 2:] * partial_derivatives.partial_dv_dx[:, :, :, 2:]
    ).sum(
        dim=-2
    )
    world_y_gradients = (
        residuals_u[:, :, :, 2:] * partial_derivatives.partial_du_dy[:, :, :, 2:]
        + residuals_v[:, :, :, 2:] * partial_derivatives.partial_dv_dy[:, :, :, 2:]
    ).sum(dim=-2)
    world_z_gradients = (
        residuals_u[:, :, :, 3:] * partial_derivatives.partial_du_dz[:, :, :, 3:]
        + residuals_v[:, :, :, 3:] * partial_derivatives.partial_dv_dz[:, :, :, 3:]
    ).sum(dim=-2)
    return torch.cat(
        [
            cx_gradients,
            cy_gradients,
            f_gradients,
            # Stack the camera gradients, by parameter
            a_gradients,
            b_gradients,
            c_gradients,
            tx_gradients,
            ty_gradients,
            tz_gradients,
            # Stack the world point gradients, per axis
            world_x_gradients,
            world_y_gradients,
            world_z_gradients,
        ],
        dim=-1,
    )
